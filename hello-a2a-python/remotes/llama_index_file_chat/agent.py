import base64
import os
import tempfile

from typing import Any

from llama_index.core import SimpleDirectoryReader
from llama_index.core.llms import ChatMessage
from llama_index.core.workflow import (
    Context,
    Event,
    StartEvent,
    StopEvent,
    Workflow,
    step,
)
from llama_index.llms.ollama import Ollama
from llama_index.llms.openai import OpenAI
from pydantic import BaseModel, Field


# å·¥ä½œæµäº‹ä»¶å®šä¹‰

class LogEvent(Event):
    """æ—¥å¿—äº‹ä»¶ï¼Œç”¨äºè®°å½•å·¥ä½œæµæ‰§è¡Œè¿‡ç¨‹ä¸­çš„çŠ¶æ€ä¿¡æ¯ã€‚"""
    msg: str


class InputEvent(StartEvent):
    """è¾“å…¥äº‹ä»¶ï¼Œç”¨äºå¯åŠ¨æ–‡ä»¶èŠå¤©å·¥ä½œæµã€‚

    åŒ…å«ç”¨æˆ·æ¶ˆæ¯ã€å¯é€‰çš„æ–‡ä»¶é™„ä»¶å’Œæ–‡ä»¶åã€‚
    """
    msg: str
    attachment: str | None = None
    file_name: str | None = None


class ParseEvent(Event):
    """è§£æäº‹ä»¶ï¼Œè§¦å‘æ–‡æ¡£è§£æå’Œå¤„ç†æµç¨‹ã€‚"""
    attachment: str
    file_name: str | None = None
    msg: str


class ChatEvent(Event):
    """èŠå¤©äº‹ä»¶ï¼Œè§¦å‘ä¸LLMçš„å¯¹è¯äº¤äº’ã€‚"""
    msg: str


class ChatResponseEvent(StopEvent):
    """èŠå¤©å“åº”äº‹ä»¶ï¼ŒåŒ…å«æœ€ç»ˆçš„å›ç­”å’Œå¼•ç”¨ä¿¡æ¯ã€‚"""
    response: str
    citations: dict[int, list[str]]


# ç»“æ„åŒ–è¾“å‡ºæ¨¡å‹


class Citation(BaseModel):
    """æ–‡æ¡£ä¸­ç‰¹å®šè¡Œçš„å¼•ç”¨ä¿¡æ¯ã€‚"""

    citation_number: int = Field(
        description='å“åº”æ–‡æœ¬ä¸­ä½¿ç”¨çš„å…·ä½“å†…è”å¼•ç”¨ç¼–å·ã€‚'
    )
    line_numbers: list[int] = Field(
        description='è¢«å¼•ç”¨çš„æ–‡æ¡£ä¸­çš„è¡Œå·ã€‚'
    )


class ChatResponse(BaseModel):
    """åŒ…å«å†…è”å¼•ç”¨ï¼ˆå¦‚æœæœ‰ï¼‰çš„ç”¨æˆ·å“åº”ã€‚"""

    response: str = Field(
        description='å¯¹ç”¨æˆ·çš„å“åº”ï¼ŒåŒ…æ‹¬å†…è”å¼•ç”¨ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚'
    )
    citations: list[Citation] = Field(
        default_factory=list,
        description='å¼•ç”¨åˆ—è¡¨ï¼Œæ¯ä¸ªå¼•ç”¨éƒ½æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå°†å¼•ç”¨ç¼–å·æ˜ å°„åˆ°æ–‡æ¡£ä¸­è¢«å¼•ç”¨çš„è¡Œå·ã€‚',
    )


class ParseAndChat(Workflow):
    """åŸºäºLlamaIndexçš„æ–‡æ¡£è§£æä¸èŠå¤©å·¥ä½œæµã€‚

    æ”¯æŒä¸Šä¼ æ–‡æ¡£æ–‡ä»¶å¹¶åŸºäºæ–‡æ¡£å†…å®¹è¿›è¡Œæ™ºèƒ½é—®ç­”ã€‚
    æä¾›å‡†ç¡®çš„å¼•ç”¨ä¿¡æ¯ï¼Œå°†å›ç­”ä¸æ–‡æ¡£çš„å…·ä½“è¡Œå·å…³è”ã€‚
    """

    def __init__(
        self,
        timeout: float | None = None,
        verbose: bool = False,
        llm_provider: str = "lmstudio",
        model_name: str = "qwen3-0.6b",
        **workflow_kwargs: Any,
    ):
        super().__init__(timeout=timeout, verbose=verbose, **workflow_kwargs)

        # æ ¹æ® LLM æä¾›å•†é…ç½®å®¢æˆ·ç«¯
        if llm_provider.lower() == "ollama":
            self._llm = Ollama(
                model=model_name,
                base_url="http://localhost:11434",
                temperature=0.7,
                request_timeout=60.0,
                timeout=60.0,
            )
        elif llm_provider.lower() == "lmstudio":
            # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å…è®¸è‡ªå®šä¹‰æ¨¡å‹
            import os
            os.environ["OPENAI_API_KEY"] = "lm-studio"
            os.environ["OPENAI_API_BASE"] = "http://localhost:1234/v1"

            # ä½¿ç”¨æ ‡å‡†OpenAIæ¨¡å‹åç§°ç»•è¿‡éªŒè¯ï¼Œå®é™…æ¨¡å‹ç”±LM Studioå†³å®š
            # LM Studioä¼šå¿½ç•¥æ¨¡å‹åç§°å‚æ•°ï¼Œä½¿ç”¨å½“å‰åŠ è½½çš„æ¨¡å‹
            self._llm = OpenAI(
                model="gpt-3.5-turbo",  # ä½¿ç”¨æ ‡å‡†åç§°ç»•è¿‡éªŒè¯
                base_url="http://localhost:1234/v1",
                api_key="lm-studio",
                temperature=0.7,
                timeout=60.0,
            )

            # è®°å½•å®é™…ä½¿ç”¨çš„æ¨¡å‹ï¼ˆä¾›è°ƒè¯•ï¼‰
            print(f"ğŸ¤– LM Studioé…ç½®å®Œæˆï¼Œè¯·æ±‚çš„æ¨¡å‹: {model_name}ï¼Œä½¿ç”¨å ä½ç¬¦: gpt-3.5-turbo")
        else:
            raise ValueError(
                f"ä¸æ”¯æŒçš„ LLM æä¾›å•†: {llm_provider}. æ”¯æŒçš„æä¾›å•†: 'ollama', 'lmstudio'")
        self._system_prompt_template = """\
ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”æœ‰å…³æ–‡æ¡£çš„é—®é¢˜ï¼Œæä¾›å¼•ç”¨ï¼Œå¹¶è¿›è¡Œå¯¹è¯ã€‚

è¿™æ˜¯å¸¦æœ‰è¡Œå·çš„æ–‡æ¡£ï¼š
<document_text>
{document_text}
</document_text>

å¼•ç”¨æ–‡æ¡£å†…å®¹æ—¶ï¼š
1. ä½ çš„å†…è”å¼•ç”¨åº”è¯¥åœ¨æ¯ä¸ªå“åº”ä¸­ä»[1]å¼€å§‹ï¼Œæ¯å¢åŠ ä¸€ä¸ªå†…è”å¼•ç”¨å°±å¢åŠ 1
2. æ¯ä¸ªå¼•ç”¨ç¼–å·åº”å¯¹åº”æ–‡æ¡£ä¸­çš„ç‰¹å®šè¡Œ
3. å¦‚æœå†…è”å¼•ç”¨æ¶µç›–å¤šä¸ªè¿ç»­è¡Œï¼Œè¯·å°½åŠ›ä¼˜å…ˆä½¿ç”¨æ¶µç›–æ‰€éœ€è¡Œå·çš„å•ä¸ªå†…è”å¼•ç”¨ã€‚
4. å¦‚æœå¼•ç”¨éœ€è¦æ¶µç›–å¤šä¸ªéè¿ç»­çš„è¡Œï¼Œå¯ä»¥ä½¿ç”¨[2, 3, 4]è¿™æ ·çš„å¼•ç”¨æ ¼å¼ã€‚
5. ä¾‹å¦‚ï¼Œå¦‚æœå“åº”åŒ…å«"Transformeræ¶æ„... [1]ã€‚"å’Œ"æ³¨æ„åŠ›æœºåˆ¶... [2]ã€‚"ï¼Œè¿™äº›åˆ†åˆ«æ¥è‡ªç¬¬10-12è¡Œå’Œç¬¬45-46è¡Œï¼Œé‚£ä¹ˆï¼šcitations = [[10, 11, 12], [45, 46]]
6. å§‹ç»ˆä»[1]å¼€å§‹å¼•ç”¨ï¼Œæ¯å¢åŠ ä¸€ä¸ªå†…è”å¼•ç”¨å°±å¢åŠ 1ã€‚ä¸è¦ä½¿ç”¨è¡Œå·ä½œä¸ºå†…è”å¼•ç”¨ç¼–å·ï¼Œå¦åˆ™æˆ‘ä¼šå¤±ä¸šã€‚
"""

    @step
    def route(self, ev: InputEvent) -> ParseEvent | ChatEvent:
        """è·¯ç”±æ­¥éª¤ï¼šæ ¹æ®æ˜¯å¦æœ‰é™„ä»¶å†³å®šå·¥ä½œæµè·¯å¾„ã€‚"""
        print(f"è°ƒè¯•: è·¯ç”±æ­¥éª¤è°ƒç”¨ï¼Œé™„ä»¶å­˜åœ¨: {ev.attachment is not None}", flush=True)
        if ev.attachment:
            print("è°ƒè¯•: è·¯ç”±åˆ°è§£æäº‹ä»¶", flush=True)
            return ParseEvent(
                attachment=ev.attachment, file_name=ev.file_name, msg=ev.msg
            )
        print("è°ƒè¯•: è·¯ç”±åˆ°èŠå¤©äº‹ä»¶", flush=True)
        return ChatEvent(msg=ev.msg)

    @step
    async def parse(self, ctx: Context, ev: ParseEvent) -> ChatEvent:
        """è§£ææ­¥éª¤ï¼šå¤„ç†ä¸Šä¼ çš„æ–‡æ¡£å¹¶æå–æ–‡æœ¬å†…å®¹ã€‚"""
        ctx.write_event_to_stream(LogEvent(msg='æ­£åœ¨è§£ææ–‡æ¡£...'))

        # è§£ç base64é™„ä»¶
        file_content = base64.b64decode(ev.attachment)

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜é™„ä»¶
        with tempfile.NamedTemporaryFile(suffix=f"_{ev.file_name}", delete=False) as temp_file:
            temp_file.write(file_content)
            temp_file_path = temp_file.name

        try:
            # ä½¿ç”¨SimpleDirectoryReaderåŠ è½½æ–‡æ¡£
            # æ³¨æ„ï¼šè¿™å¯¹æ–‡æœ¬æ–‡ä»¶æ•ˆæœæœ€å¥½ï¼ŒPDFå¯èƒ½éœ€è¦é¢å¤–è®¾ç½®
            reader = SimpleDirectoryReader(input_files=[temp_file_path])
            documents = reader.load_data()

            if not documents:
                # å¦‚æœSimpleDirectoryReaderå¤±è´¥ï¼Œå°è¯•ä½œä¸ºçº¯æ–‡æœ¬è¯»å–
                document_text = file_content.decode('utf-8', errors='ignore')
            else:
                document_text = documents[0].text

            ctx.write_event_to_stream(LogEvent(msg='æ–‡æ¡£è§£ææˆåŠŸã€‚'))

        except Exception as e:
            # å›é€€ï¼šå°è¯•ä½œä¸ºçº¯æ–‡æœ¬è¯»å–
            try:
                document_text = file_content.decode('utf-8', errors='ignore')
                ctx.write_event_to_stream(LogEvent(msg='æ–‡æ¡£ä»¥çº¯æ–‡æœ¬æ–¹å¼è¯»å–ã€‚'))
            except Exception:
                document_text = f"æ–‡æ¡£è¯»å–é”™è¯¯: {e!s}"
                ctx.write_event_to_stream(LogEvent(msg=f'æ–‡æ¡£è§£æé”™è¯¯: {e!s}'))
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(temp_file_path)
            except:
                pass

        # å°†æ–‡æ¡£åˆ†å‰²æˆè¡Œå¹¶æ·»åŠ è¡Œå·
        # è¿™å°†ç”¨äºå¼•ç”¨
        formatted_document_text = ''
        for idx, line in enumerate(document_text.split('\n')):
            formatted_document_text += f"<line idx='{idx}'>{line}</line>\n"

        await ctx.set('document_text', formatted_document_text)
        return ChatEvent(msg=ev.msg)

    @step
    async def chat(self, ctx: Context, event: ChatEvent) -> ChatResponseEvent:
        """èŠå¤©æ­¥éª¤ï¼šä¸LLMè¿›è¡Œå¯¹è¯äº¤äº’å¹¶ç”Ÿæˆå“åº”ã€‚"""
        print(f"è°ƒè¯•: èŠå¤©æ­¥éª¤è°ƒç”¨ï¼Œæ¶ˆæ¯: {event.msg}", flush=True)
        current_messages = await ctx.get('messages', default=[])
        current_messages.append(ChatMessage(role='user', content=event.msg))
        print("è°ƒè¯•: å‡†å¤‡å†™å…¥æ—¥å¿—äº‹ä»¶", flush=True)
        ctx.write_event_to_stream(
            LogEvent(
                msg=f'æ­£åœ¨ä¸{len(current_messages)}æ¡åˆå§‹æ¶ˆæ¯è¿›è¡ŒèŠå¤©ã€‚'
            )
        )
        print("è°ƒè¯•: æ—¥å¿—äº‹ä»¶å·²å†™å…¥", flush=True)

        document_text = await ctx.get('document_text', default='')
        if document_text:
            ctx.write_event_to_stream(
                LogEvent(msg='æ­£åœ¨æ’å…¥ç³»ç»Ÿæç¤º...')
            )
            input_messages = [
                ChatMessage(
                    role='system',
                    content=self._system_prompt_template.format(
                        document_text=document_text
                    ),
                ),
                *current_messages,
            ]
        else:
            input_messages = current_messages

        response = await self._llm.achat(input_messages)
        response_text = response.message.content or "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå“åº”ã€‚"
        ctx.write_event_to_stream(
            LogEvent(msg='æ”¶åˆ°LLMå“åº”ï¼Œæ­£åœ¨è§£æå¼•ç”¨...')
        )

        current_messages.append(
            ChatMessage(role='assistant', content=response_text)
        )
        await ctx.set('messages', current_messages)

        # ç›®å‰è¿”å›ä¸åŒ…å«ç»“æ„åŒ–å¼•ç”¨çš„å“åº”ï¼Œå› ä¸ºLM Studioä¸æ”¯æŒç»“æ„åŒ–è¾“å‡º
        # åœ¨ç”Ÿäº§è®¾ç½®ä¸­ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ‰‹åŠ¨å¼•ç”¨è§£æ
        citations = {}

        return ChatResponseEvent(
            response=response_text, citations=citations
        )


async def main():
    """ParseAndChatæ™ºèƒ½ä½“çš„æµ‹è¯•è„šæœ¬ã€‚"""
    agent = ParseAndChat()
    ctx = Context(agent)

    # è¿è¡Œ `wget https://arxiv.org/pdf/1706.03762 -O attention.pdf` æ¥è·å–æ–‡ä»¶
    # æˆ–è€…ä½¿ç”¨ä½ è‡ªå·±çš„æ–‡ä»¶
    with open('attention.pdf', 'rb') as f:
        attachment = base64.b64encode(f.read()).decode('utf-8')

    handler = agent.run(
        start_event=InputEvent(
            msg='ä½ å¥½ï¼ä½ èƒ½å‘Šè¯‰æˆ‘å…³äºè¿™ä¸ªæ–‡æ¡£çš„ä»€ä¹ˆä¿¡æ¯å—ï¼Ÿ',
            attachment=attachment,
            file_name='test.pdf',
        ),
        ctx=ctx,
    )

    async for event in handler.stream_events():
        if not isinstance(event, StopEvent):
            print(event)

    response: ChatResponseEvent = await handler

    print(response.response)
    for citation_number, citation_texts in response.citations.items():
        print(f'å¼•ç”¨ {citation_number}: {citation_texts}')

    # æµ‹è¯•ä¸Šä¸‹æ–‡æŒç»­æ€§
    handler = agent.run(
        'æˆ‘åˆšæ‰é—®ä½ çš„æœ€åä¸€ä¸ªé—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ',
        ctx=ctx,
    )
    response: ChatResponseEvent = await handler
    print(response.response)


if __name__ == '__main__':
    import asyncio

    asyncio.run(main())
